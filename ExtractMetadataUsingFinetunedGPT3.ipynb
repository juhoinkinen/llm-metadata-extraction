{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4a32f6",
   "metadata": {},
   "source": [
    "# Extract metadata from PDF files using fine-tuned GPT3 language model\n",
    "\n",
    "This notebook will demonstrate how we can extract Dublin Core style metadata about PDF documents, in this case doctoral theses from four Finnish universities (Ã…bo Akademi, University of Turku, University of Vaasa and Lappeenranta University of Technology), using only the raw text from the first few pages of the PDF.\n",
    "\n",
    "The set of 192 documents will be split into two subsets (train: 149, test: 43). We will extract the text from around 5 pages of text, aiming for 500 to 700 words. The corresponding metadata, which has been exported from DSpace repositories of the universities, is represented in a simple textual \"key: value\" format, which should be easy enough for a language model to handle. The train set is used to create a data set which will then be used to fine-tune a GPT model. Subsequently the model can be used to generate similar metadata for unseen documents from the test set.\n",
    "\n",
    "For this experiment, an OpenAI API access key is required. It can be generated after registering an user account (the same account can be used for e.g. ChatGPT). The API key has to be stored in an environment variable `OPENAI_API_KEY` before starting this notebook. The finetuning will cost around \\\\$5 USD and generating new metadata with the API also has a small cost, but currently every account gets a free \\\\$18 credit from OpenAI which is plenty for this experiment even with a few iterations.\n",
    "\n",
    "This notebook depends on a few Python libraries, which are listed in `requirements.txt`. See the README for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c63d21",
   "metadata": {},
   "source": [
    "## Test the connection and API key\n",
    "\n",
    "Make sure it's possible to use the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ff64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(id='cmpl-8NfzPEOGuGfXo95HaW7XaE7lw9dP7', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=\". I'm gonna say this is\")], created=1700653643, model='davinci-002', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=5, total_tokens=12))\n",
      ". I'm gonna say this is\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# read the OpenAI API key from an environment variable\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# test the API connection by making a simple request\n",
    "response = openai.completions.create(model=\"davinci-002\", prompt=\"Say this is a test\", temperature=0, max_tokens=7)\n",
    "print(response)\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db4c29",
   "metadata": {},
   "source": [
    "## Prepare the data set\n",
    "\n",
    "Extract metadata and PDF text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f64d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some settings for the metadata extraction\n",
    "\n",
    "import glob\n",
    "\n",
    "MAXPAGES = 5  # how many pages of text to extract (maximum)\n",
    "MARGIN = 2  # how many more pages to look at, in case we can't find text from the first ones\n",
    "TEXT_MIN = 500  # how many words to aim for (minimum)\n",
    "TEXT_LIMIT = 700  # upper limit on # of words\n",
    "\n",
    "# files containing metadata about doctoral theses documents, exported from DSpace repositories\n",
    "METADATAFILES = glob.glob(\"dspace/*-doctheses.xml\")\n",
    "\n",
    "# metadata fields we are interested in (corresponding to fields used in DSpace)\n",
    "# syntax: \"fieldname\" or \"fieldname/qualifier\"\n",
    "METADATAFIELDS = \"\"\"\n",
    "title\n",
    "title/alternative\n",
    "contributor/faculty\n",
    "contributor/author\n",
    "contributor/organization\n",
    "contributor/opponent\n",
    "contributor/supervisor\n",
    "contributor/reviewer\n",
    "publisher\n",
    "date/issued\n",
    "relation/issn\n",
    "relation/isbn\n",
    "relation/ispartofseries\n",
    "relation/numberinseries\n",
    "\"\"\".strip().split()\n",
    "\n",
    "# identifiers of documents that will form the test set\n",
    "# these have been selected to correspond with a demo application for the same purpose\n",
    "TEST_SET_IDS = \"\"\"\n",
    "handle_10024_181378\n",
    "handle_10024_181284\n",
    "handle_10024_181280\n",
    "handle_10024_181229\n",
    "handle_10024_181227\n",
    "handle_10024_181210\n",
    "handle_10024_181206\n",
    "handle_10024_181139\n",
    "handle_10024_181073\n",
    "handle_10024_181025\n",
    "handle_10024_181001\n",
    "handle_10024_163335\n",
    "handle_10024_163304\n",
    "handle_10024_163298\n",
    "handle_10024_163277\n",
    "handle_10024_163276\n",
    "handle_10024_163263\n",
    "handle_10024_163258\n",
    "handle_10024_163257\n",
    "handle_10024_163057\n",
    "handle_10024_163056\n",
    "handle_10024_162878\n",
    "handle_10024_11364\n",
    "handle_10024_11363\n",
    "handle_10024_11348\n",
    "handle_10024_11207\n",
    "handle_10024_10928\n",
    "handle_10024_10620\n",
    "handle_10024_10614\n",
    "handle_10024_10443\n",
    "handle_10024_10432\n",
    "handle_10024_10254\n",
    "handle_10024_152922\n",
    "handle_10024_152903\n",
    "handle_10024_152904\n",
    "handle_10024_152860\n",
    "handle_10024_152862\n",
    "handle_10024_152853\n",
    "handle_10024_152854\n",
    "handle_10024_152855\n",
    "handle_10024_152852\n",
    "handle_10024_152846\n",
    "handle_10024_152836\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed0cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping page 5 of docs/www.utupub.fi_handle_10024_153232.pdf: text would become too long\n",
      "skipping page 5 of docs/www.utupub.fi_handle_10024_153200.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_182724.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_182724.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_182159.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_182159.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181975.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181975.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 7 of docs/www.doria.fi_handle_10024_181902.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181847.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181727.pdf: text would become too long\n",
      "no file element found (id: https://www.doria.fi/handle/10024/181511), skipping\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 6 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 7 of docs/www.doria.fi_handle_10024_181280.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181227.pdf: text would become too long\n",
      "skipping page 5 of docs/www.doria.fi_handle_10024_181073.pdf: text would become too long\n",
      "no file element found (id: https://lutpub.lut.fi/handle/10024/163756), skipping\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13521.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13449.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13420.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13378.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13377.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13257.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13205.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13177.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13169.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13167.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13159.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13153.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13152.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13119.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_13116.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12958.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12619.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12481.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12480.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12300.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12272.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_12259.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11784.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11593.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11584.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11561.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11491.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11407.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11363.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11348.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_11207.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10614.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10443.pdf: text would become too long\n",
      "skipping page 5 of docs/osuva.uwasa.fi_handle_10024_10432.pdf: text would become too long\n",
      "train set size: 149\n",
      "test set size: 43\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "from lxml import etree\n",
    "import requests\n",
    "import os.path\n",
    "from pypdf import PdfReader\n",
    "import glob\n",
    "\n",
    "# train set: document identifiers, text (x) and metadata (y)\n",
    "train_ids = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "# test set: document identifiers, text (x) and metadata (y)\n",
    "test_ids = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "def extract_metadata(doc_item):\n",
    "    \"\"\"extract the metadata as a list of (key, value) tuples from an etree element representing a document\"\"\"\n",
    "    metadata = []\n",
    "    for fldname in METADATAFIELDS:\n",
    "        if '/' in fldname:\n",
    "            fld, qualifier = fldname.split('/')\n",
    "            for val in doc_item.findall(f\"metadata[@element='{fld}'][@qualifier='{qualifier}']\"):\n",
    "                if fld == 'date':\n",
    "                    metadata.append((fldname, val.text[:4]))  # only the year\n",
    "                else:\n",
    "                    metadata.append((fldname, val.text))\n",
    "        else:\n",
    "            for val in doc_item.findall(f\"metadata[@element='{fldname}'][@qualifier='']\"):\n",
    "                metadata.append((fldname, val.text))\n",
    "    return metadata\n",
    "\n",
    "def id_to_fn(identifier):\n",
    "    \"\"\"convert a URI identifier to a simpler string we can use as a filename for the PDF\"\"\"\n",
    "    return 'docs/' + identifier.replace('https://', '').replace('/','_') + \".pdf\"\n",
    "\n",
    "def download(file_url, identifier):\n",
    "    \"\"\"download a PDF file, with the given identifier, from the given URL (unless this was done already)\n",
    "    and return a path to the PDF file\"\"\"\n",
    "    path = id_to_fn(identifier)\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        return path\n",
    "\n",
    "    response = requests.get(file_url)\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "        print(f\"wrote {file_url} as {path}\")\n",
    "    return path\n",
    "\n",
    "def extract_text(fn):\n",
    "    \"\"\"extract and return the first few pages of text from the given PDF file\"\"\"\n",
    "    reader = PdfReader(fn)\n",
    "    texts = []\n",
    "    extracted_pages = 0\n",
    "    extracted_length = 0\n",
    "    for idx, page in enumerate(reader.pages[:MAXPAGES + MARGIN]):\n",
    "        text = page.extract_text()\n",
    "        text_length = len(text.strip().split())\n",
    "        if extracted_length + text_length < TEXT_LIMIT:\n",
    "            texts.append(text)\n",
    "            extracted_length += text_length\n",
    "            extracted_pages += 1\n",
    "        else:\n",
    "            print(f\"skipping page {idx+1} of {fn}: text would become too long\")\n",
    "        if extracted_pages >= MAXPAGES or extracted_length >= TEXT_MIN:\n",
    "            break\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "def is_test_doc(identifier):\n",
    "    \"\"\"return True iff the given identifier belongs to the test set\"\"\"\n",
    "    shortid = 'handle' + identifier.split('handle')[1].replace('/', '_')\n",
    "    return shortid in TEST_SET_IDS\n",
    "\n",
    "# Read all the metadata files, extract the DSpace metadata, download the PDFs and extract text from them\n",
    "# into the train_* and test_* lists\n",
    "for fn in METADATAFILES:\n",
    "    tree = etree.parse(fn)\n",
    "    for item in tree.findall('item'):\n",
    "        try:\n",
    "            identifier = item.find(\"metadata[@element='identifier'][@qualifier='uri']\").text\n",
    "        except AttributeError:\n",
    "            print(\"no identifier found, skipping\")\n",
    "            continue\n",
    "        try:\n",
    "            file_url = item.find('file').text\n",
    "        except AttributeError:\n",
    "            print(f\"no file element found (id: {identifier}), skipping\")\n",
    "            continue\n",
    "            print(f\"skipping test document {identifier}\")\n",
    "            continue\n",
    "        path = download(file_url, identifier)\n",
    "        text = extract_text(path)\n",
    "        metadata = extract_metadata(item)\n",
    "        if is_test_doc(identifier):\n",
    "            test_ids.append(identifier)\n",
    "            test_x.append(text)\n",
    "            test_y.append(metadata)\n",
    "        else:\n",
    "            train_ids.append(identifier)\n",
    "            train_x.append(text)\n",
    "            train_y.append(metadata)\n",
    "\n",
    "print(f\"train set size: {len(train_ids)}\")\n",
    "print(f\"test set size: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1daf3fc",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Prepare a fine-tuning dataset and use it to fine-tune a GPT3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f9952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote fine-tuning data set into file fine-tune.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prepare fine-tuning dataset\n",
    "import json\n",
    "\n",
    "PROMPT_SUFFIX = '\\n\\n###\\n\\n'\n",
    "COMPLETION_STOP = '\\n###'\n",
    "TRAINFILE = 'fine-tune.jsonl'\n",
    "\n",
    "def metadata_to_text(metadata):\n",
    "    \"\"\"convert the metadata tuple to text with key: value pairs\"\"\"\n",
    "    return \"\\n\".join([f\"{fld}: {val}\" for fld, val in metadata])\n",
    "\n",
    "def create_sample(text, metadata):\n",
    "    \"\"\"create a fine-tuning sample from text and metadata about a single document\"\"\"\n",
    "    return {'prompt': text + PROMPT_SUFFIX,\n",
    "            'completion': metadata_to_text(metadata) + COMPLETION_STOP}\n",
    "\n",
    "with open(TRAINFILE, 'w') as outf:\n",
    "    for text, metadata in zip(train_x, train_y):\n",
    "        sample = create_sample(text, metadata)\n",
    "        print(json.dumps(sample), file=outf)\n",
    "\n",
    "print(f\"wrote fine-tuning data set into file {TRAINFILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 149 prompt-completion pairs\n",
      "- All prompts end with suffix `\\n\\n###\\n\\n`\n",
      "- All completions start with prefix `title: `. Most of the time you should only add the output data into the completion, without any prefix\n",
      "- All completions end with suffix `\\n###`\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Recommended] Remove prefix `title: ` from all completions [Y/n]: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional:\n",
    "# Check that the fine-tuning data set is OK using the prepare_data tool.\n",
    "# It will complain that all completions start with the same \"title:\" prefix, this can be ignored.\n",
    "# NOTE: The command has to be interrupted by pressing the stop button in Jupyter.\n",
    "!openai tools fine_tunes.prepare_data -f fine-tune.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-7xlI2o70vyBn0pmu5qr9VpJn', bytes=643486, created_at=1700655246, filename='fine-tune.jsonl', object='file', purpose='fine-tune', status='uploaded', status_details=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload training data\n",
    "\n",
    "upload_response = openai.files.create(\n",
    "    file=open(TRAINFILE, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "trainfile_id = upload_response.id\n",
    "upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-03qWkJDNvGiWPkUS8gKj6Po6', created_at=1700655410, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='davinci-002', object='fine_tuning.job', organization_id='org-5QEUW2DacClOLTNQvTEKMHdV', result_files=[], status='validating_files', trained_tokens=None, training_file='file-7xlI2o70vyBn0pmu5qr9VpJn', validation_file=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the actual finetuning via the API. This can take a while, there can be a long queue.\n",
    "\n",
    "openai.fine_tuning.jobs.create(\n",
    "    training_file=trainfile_id,\n",
    "    model=\"davinci-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FineTuningJobEvent(id='ftevent-DRZk9aSjRgegzl4HSqlX336k', created_at=1700655698, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-ou3WAE3Cv5WKKcGk57tnEWlU', created_at=1700655695, level='info', message='New fine-tuned model created: ft:davinci-002:personal::8NgWUbLM', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-4rBvsUGvACde4EwYDBsopslF', created_at=1700655685, level='info', message='Step 441/447: training loss=0.03', object='fine_tuning.job.event', data={'step': 441, 'train_loss': 0.028055638074874878, 'train_mean_token_accuracy': 0.9852216839790344}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-X5TytJwlt7mLKBfS11MiZDLA', created_at=1700655682, level='info', message='Step 431/447: training loss=0.03', object='fine_tuning.job.event', data={'step': 431, 'train_loss': 0.031480707228183746, 'train_mean_token_accuracy': 0.9888476133346558}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-odAL98Xgvs9KvYUy7xWRVjWB', created_at=1700655677, level='info', message='Step 421/447: training loss=0.02', object='fine_tuning.job.event', data={'step': 421, 'train_loss': 0.019590716809034348, 'train_mean_token_accuracy': 0.9946808218955994}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-sRvW2NVHTY0Z974xpLOmJLO3', created_at=1700655675, level='info', message='Step 411/447: training loss=0.01', object='fine_tuning.job.event', data={'step': 411, 'train_loss': 0.006364763248711824, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-mnaULeIIbW5beuCpbiKMnqiC', created_at=1700655672, level='info', message='Step 401/447: training loss=0.00', object='fine_tuning.job.event', data={'step': 401, 'train_loss': 1.6725980458431877e-05, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-bkoc8nr2mOokgLfljg9S7vxh', created_at=1700655667, level='info', message='Step 391/447: training loss=0.30', object='fine_tuning.job.event', data={'step': 391, 'train_loss': 0.2988974153995514, 'train_mean_token_accuracy': 0.9433962106704712}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-33YFDT9CKt76qWC3DPpVBqDt', created_at=1700655664, level='info', message='Step 381/447: training loss=0.00', object='fine_tuning.job.event', data={'step': 381, 'train_loss': 0.0004882121575064957, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-5E8YDipIRnQFebNr5JHwPJdy', created_at=1700655659, level='info', message='Step 371/447: training loss=0.06', object='fine_tuning.job.event', data={'step': 371, 'train_loss': 0.06012587621808052, 'train_mean_token_accuracy': 0.988950252532959}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-NJnjgTTbx8PO62Wtq44dEqRb', created_at=1700655654, level='info', message='Step 361/447: training loss=0.01', object='fine_tuning.job.event', data={'step': 361, 'train_loss': 0.007715165615081787, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-WXArsaKTOdKlwWZueuk7U3Ij', created_at=1700655652, level='info', message='Step 351/447: training loss=0.12', object='fine_tuning.job.event', data={'step': 351, 'train_loss': 0.12305770069360733, 'train_mean_token_accuracy': 0.982300877571106}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-U16jJFxMSRsc9fUOcuHevXWy', created_at=1700655647, level='info', message='Step 341/447: training loss=0.15', object='fine_tuning.job.event', data={'step': 341, 'train_loss': 0.15212750434875488, 'train_mean_token_accuracy': 0.97826087474823}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-j5pk9myzJn0ZDxZyAaGDKPkf', created_at=1700655644, level='info', message='Step 331/447: training loss=0.35', object='fine_tuning.job.event', data={'step': 331, 'train_loss': 0.34505435824394226, 'train_mean_token_accuracy': 0.915730357170105}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-FXXu9CcGe1nMeNBdIEHUzxGW', created_at=1700655640, level='info', message='Step 321/447: training loss=0.00', object='fine_tuning.job.event', data={'step': 321, 'train_loss': 2.227554068667814e-05, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-fpX8pmALiiKnYDB2zkRxxsHX', created_at=1700655636, level='info', message='Step 311/447: training loss=0.05', object='fine_tuning.job.event', data={'step': 311, 'train_loss': 0.05124074965715408, 'train_mean_token_accuracy': 0.9861111044883728}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-umIZSzuUGK7cdLFtx9vDDOrj', created_at=1700655631, level='info', message='Step 301/447: training loss=0.00', object='fine_tuning.job.event', data={'step': 301, 'train_loss': 1.610044091648888e-05, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-zBrezYCXYv7xBSic4v5tJ8Wk', created_at=1700655629, level='info', message='Step 291/447: training loss=0.45', object='fine_tuning.job.event', data={'step': 291, 'train_loss': 0.4476657211780548, 'train_mean_token_accuracy': 0.9105263352394104}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-WEcicbn18UN5bY83CWpPfD30', created_at=1700655624, level='info', message='Step 281/447: training loss=0.09', object='fine_tuning.job.event', data={'step': 281, 'train_loss': 0.09101799875497818, 'train_mean_token_accuracy': 0.9626865386962891}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-SnrfBRPgQr0SYWIGlKPRnFPx', created_at=1700655622, level='info', message='Step 271/447: training loss=0.05', object='fine_tuning.job.event', data={'step': 271, 'train_loss': 0.054238006472587585, 'train_mean_token_accuracy': 0.9899497628211975}, type='metrics')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_job_id = openai.fine_tuning.jobs.list(limit=10).data[0].id\n",
    "openai.fine_tuning.jobs.list_events(fine_tuning_job_id=fine_tuning_job_id, limit=20).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9376e9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:davinci-002:personal::8NgWUbLM'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the model name from above fine tuning job\n",
    "\n",
    "model_name = openai.fine_tuning.jobs.retrieve(fine_tuning_job_id).fine_tuned_model\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285709c",
   "metadata": {},
   "source": [
    "## Test the fine-tuned model\n",
    "\n",
    "Give the model some documents from the test set that it has never seen before and see what kind of metadata it can extract. Compare that to the manually created metadata of the same documents, extracted from DSpace systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8cba90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.utupub.fi/handle/10024/152860\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Essays on income inequality and financial incentives to work\n",
      "contributor/faculty: fi=Turun kauppakorkeakoulu|en=Turku School of Economics|\n",
      "contributor/author: Ollonqvist, Joonas\n",
      "publisher: fi=Turun yliopisto. Turun kauppakorkeakoulu|en=University of Turku, Turku School of Economics|\n",
      "date/issued: 2021\n",
      "relation/issn: 2343-3167\n",
      "relation/ispartofseries: Turun yliopiston julkaisuja - Annales Universitatis Turkuensis, Ser E: Oeconomica\n",
      "relation/numberinseries: 82\n",
      "---\n",
      "Generated metadata:\n",
      "title: Essays on income inequality and financial incentives to work\n",
      "contributor/faculty: fi=Turun kauppakorkeakoulu|en=Turku School of Economics|\n",
      "contributor/author: Ollonqvist, Joonas\n",
      "publisher: fi=Turun yliopisto. Turun kauppakorkeakoulu|en=University of Turku, Turku School of Economics|\n",
      "date/issued: 2022\n",
      "relation/issn: 2343-3167\n",
      "relation/ispartofseries: Turun yliopiston julkaisua - Annales Universitatis Turkuensis, Ser. E: Oeconomica\n",
      "relation/numberinseries: 82\n",
      "\n",
      "https://www.utupub.fi/handle/10024/152852\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Run-time management of many-core SoCs: A communication-centric approach\n",
      "contributor/faculty: fi=Teknillinen tiedekunta|en=Faculty of Technology|\n",
      "contributor/author: Fattah, Mohammad\n",
      "publisher: fi=Turun yliopisto|en=University of Turku|\n",
      "date/issued: 2021\n",
      "relation/issn: 2736-9684\n",
      "relation/ispartofseries: Turun yliopiston julkaisua - Annales Universitatis Turkuensis, Ser. F: Technica - Informatica\n",
      "relation/numberinseries: 7\n",
      "---\n",
      "Generated metadata:\n",
      "title: Run-time management of many-core SoCs : a communication-centric approach\n",
      "contributor/faculty: fi=Teknillinen tiedekunta|en=Faculty of Technology|\n",
      "contributor/author: Fattah, Mohammad\n",
      "publisher: fi=Turun yliopisto|en=University of Turku|\n",
      "date/issued: 2022\n",
      "relation/issn: 2736-9684\n",
      "relation/ispartofseries: Turun yliopiston julkaisua - Annales Universitatis Turkuensis, Ser. F: Technica - Informatica\n",
      "relation/numberinseries: 7\n",
      "\n",
      "https://www.doria.fi/handle/10024/181280\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Production and testing of magnesium carbonate hydrates for thermal energy storage (TES) application\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten fÃ¶r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Erlund, Rickard\n",
      "contributor/opponent: Professor Brian Elmegaard, Technical University of Denmark, Lyngby, Denmark\n",
      "contributor/supervisor: Professor Ron Zevenhoven, Ã…bo Akademi University, Turku\n",
      "publisher: Ã…bo Akademi University\n",
      "date/issued: 2021\n",
      "---\n",
      "Generated metadata:\n",
      "title: Production and testing of magnesium carbonate hydrates for thermal energy storage (TES) application\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten fÃ¶r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Erlund, Rickard\n",
      "contributor/opponent: Professor Brian Elmegaard, Technical University of Denmark, Lyngby\n",
      "contributor/supervisor: Professor Ron Zevenhoven, Ã…bo Akademi University, Turku\n",
      "publisher: Ã…bo Akademi University\n",
      "date/issued: 2021\n",
      "\n",
      "https://www.doria.fi/handle/10024/181139\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Coulometric Transduction Method for Solid-Contact Ion-Selective Electrodes\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten fÃ¶r naturvetenskaper och teknik\n",
      "contributor/faculty: Luonnontieteiden ja tekniikan tiedekunta\n",
      "contributor/author: Han, Tingting\n",
      "contributor/opponent: Professor Agata Michalska, University of Warsaw, Warsaw, Poland\n",
      "contributor/supervisor: Professor Johan Bobacka, Ã…bo Akademi University, Ã…bo\n",
      "contributor/supervisor: Dr. Ulriika Mattinen, Ã…bo Akademi University, Ã…bo\n",
      "contributor/supervisor: Docent Zekra Mousavi, Ã…bo Akademi University, Ã…bo\n",
      "publisher: Ã…bo Akademi University\n",
      "date/issued: 2021\n",
      "---\n",
      "Generated metadata:\n",
      "title: Coulometric Transduction Method for Solid-Contact Ion-Selective Electrodes\n",
      "contributor/faculty: Faculty of Science and Engineering\n",
      "contributor/faculty: Fakulteten fÃ¶r naturvetenskaper och matematik\n",
      "contributor/faculty: Luonnontieteiden ja matematiikan tiedekunta\n",
      "contributor/author: Han, Tingting\n",
      "contributor/opponent: Professor Agata Michalska, University of Warsaw, Warsaw, Poland\n",
      "contributor/supervisor: Professor Johan Bobacka, Ã…bo Akademi University, Turku\n",
      "contributor/supervisor: Docent Ulriika Mattinen, Ã…bo Akademi University, Turku\n",
      "publisher: Ã…bo Akademi University\n",
      "date/issued: 2021\n",
      "\n",
      "https://lutpub.lut.fi/handle/10024/163304\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Responsible business practices in internationalized SMEs\n",
      "contributor/faculty: fi=School of Business and Management|en=School of Business and Management|\n",
      "contributor/author: Uzhegova, Maria\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Rialp-Criado, Alex\n",
      "contributor/reviewer: Rialp-Criado, Alex\n",
      "contributor/reviewer: Andersson, Svante\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "---\n",
      "Generated metadata:\n",
      "title: Responsible business practices in internationalized SMEs\n",
      "contributor/faculty: fi=School of Business and Management|en=School of Business and Management|\n",
      "contributor/author: Uzhegova, Maria\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Rialp-Criado, Alex\n",
      "contributor/reviewer: Rialp-Criado, Alex\n",
      "contributor/reviewer: Andersson, Svante\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "\n",
      "https://lutpub.lut.fi/handle/10024/163258\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Life cycle cost-driven design for additive manufacturing : the frontier to sustainable manufacturing in laser-based powder bed fusion\n",
      "contributor/faculty: fi=School of Energy Systems|en=School of Energy Systems|\n",
      "contributor/author: Nyamekye, Patricia\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Hryha, Eduard\n",
      "contributor/reviewer: Hryha, Eduard\n",
      "contributor/reviewer: Mazzi, Anna\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "---\n",
      "Generated metadata:\n",
      "title: Life cycle cost-driven design for additive manufacturing: the frontier to sustainable manufacturing in laser-based powder bed fusion\n",
      "contributor/faculty: fi=School of Energy Systems|en=School of Energy Systems|\n",
      "contributor/author: Nyamekye, Patricia\n",
      "contributor/organization: Lappeenrannan-Lahden teknillinen yliopisto LUT\n",
      "contributor/organization: Lappeenranta-Lahti University of Technology LUT\n",
      "contributor/opponent: Hryha, Eduard\n",
      "contributor/reviewer: Hryha, Eduard\n",
      "contributor/reviewer: Mazzi, Anna\n",
      "publisher: Lappeenranta-Lahti University of Technology LUT\n",
      "date/issued: 2021\n",
      "relation/issn: 1456-4491\n",
      "relation/ispartofseries: Acta Universitatis Lappeenrantaensis\n",
      "\n",
      "https://osuva.uwasa.fi/handle/10024/11364\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Knowledge management approach for sustainable waste management: evolving a conceptual framework\n",
      "title/alternative: Tietojohtamisen lÃ¤hestymistapa kestÃ¤vÃ¤Ã¤n jÃ¤tehuoltoon: kÃ¤sitteellisen viitekehyksen kehittÃ¤minen\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikkÃ¶|en=School of Technology and Innovations|\n",
      "contributor/author: Obule-Abila, Beatrice\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-922-8\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 448\n",
      "---\n",
      "Generated metadata:\n",
      "title: Knowledge management approach for sustainable waste management : evolving a conceptual framework\n",
      "title/alternative: Tietojohtamisen lÃ¤hestymistapa kestÃ¤vÃ¤Ã¤n jÃ¤tehuoltoon : kÃ¤sitteellisen viitekehyksen kehittÃ¤minen\n",
      "contributor/faculty: fi=Tekniikan ja innovaatiojohtamisen yksikkÃ¶|en=School of Technology and Innovations|\n",
      "contributor/author: Obule-Abila, Beatrice\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-922-8\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 448\n",
      "\n",
      "https://osuva.uwasa.fi/handle/10024/10620\n",
      "---\n",
      "DSpace metadata:\n",
      "title: Hyvinvoinnin toivottu tulevaisuus : tarkastelussa kompleksisuus, antisipaatio ja osallisuus\n",
      "title/alternative: The desired future of well-being : examining complexity, anticipation and participation\n",
      "contributor/faculty: fi=Johtamisen yksikkÃ¶|en=School of Management|\n",
      "contributor/author: Pernaa, Hanna-Kaisa\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-909-9\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 443\n",
      "---\n",
      "Generated metadata:\n",
      "title: Hyvinvoinnin toivottu tulevaisuus : tarkastelussa kompleksisuus, antisipaatio ja osallisuus\n",
      "title/alternative: The desired future of well-being : examining complexity, anticipation and participation\n",
      "contributor/faculty: fi=Johtamisen yksikkÃ¶|en=School of Management|\n",
      "contributor/author: Pernaa, Hanna-Kaisa\n",
      "contributor/organization: fi=Vaasan yliopisto|en=University of Vaasa|\n",
      "publisher: Vaasan yliopisto\n",
      "date/issued: 2020\n",
      "relation/issn: 2323-9123\n",
      "relation/issn: 0355-2667\n",
      "relation/isbn: 978-952-476-909-9\n",
      "relation/ispartofseries: Acta Wasaensia\n",
      "relation/numberinseries: 443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_completions(text):\n",
    "    response = openai.completions.create(model=model_name,\n",
    "                                    prompt=text + PROMPT_SUFFIX,\n",
    "                                    temperature=0,  # no fooling around!\n",
    "                                    max_tokens=500, # should be plenty\n",
    "                                    stop=[COMPLETION_STOP])  # stop at ###\n",
    "    return response.choices[0].text\n",
    "\n",
    "# test it with some sample documents from the test set\n",
    "for idx in (3,8,13,18,23,28,33,38):\n",
    "    identifier = test_ids[idx]\n",
    "    text = test_x[idx]\n",
    "    metadata = test_y[idx]\n",
    "    print(identifier)\n",
    "    print(\"---\")\n",
    "    print(\"DSpace metadata:\")\n",
    "    print(metadata_to_text(metadata))\n",
    "    print(\"---\")\n",
    "    print(\"Generated metadata:\")\n",
    "    gen_metadata = get_completions(text).strip()\n",
    "    print(gen_metadata)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd50b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
